{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetArticlesFromDateRange(startDate, endDate, dateToCheck):\n",
    "    castedDateToCheck = datetime.datetime.strptime(dateToCheck, '%Y-%m-%d').date()\n",
    "    if castedDateToCheck >= startDate and castedDateToCheck < endDate:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class article(object):\n",
    "    def __init__(self, date, information):\n",
    "        self.Date = date\n",
    "        self.Information = information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Got the medical documents from:\n",
    "https://github.com/socd06/medical-nlp <br />\n",
    "There's another tool someone made to get all medical terminologies on that link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMedicalTerms(filePath): \n",
    "    \n",
    "    lines = []\n",
    "    with open(filePath) as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip().lower())\n",
    "            \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the LDA Model\n",
    "https://radimrehurek.com/gensim/models/ldaseqmodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaSeqModel\n",
    "\n",
    "def GetTopicModel(corpus, id2word,dictionary, numberOfTopics = 10, chunkSize=2000):\n",
    "\n",
    "    temp = dictionary[0]  \n",
    "    \n",
    "    return LdaSeqModel(corpus=corpus,\n",
    "                       time_slice=[2330, 5992, 9234],#,9486,8986,8470,9178,9325,8368,7284,8857,5059],\n",
    "                       num_topics=numberOfTopics,\n",
    "                       id2word=id2word,\n",
    "                       chunksize=chunkSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the rule associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from datetime import datetime\n",
    "import pandas as pd  \n",
    "from IPython.display import HTML \n",
    "\n",
    "def PrintRuleAssociation(support, confidence, bow, listOfDocs, lift, length = None):   \n",
    "    porter = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    if len(bow) == 0:\n",
    "        print('No topics found')\n",
    "        return\n",
    "    \n",
    "    rules = apriori(bow,\n",
    "                    min_support= support,\n",
    "                    min_confidence= confidence,\n",
    "                    min_lift= lift,\n",
    "                    max_length= length)\n",
    "    \n",
    "    results = list(rules)\n",
    "    \n",
    "    df = pd.DataFrame(columns=('Left Hand Side',\n",
    "                               'Right Hand Side',\n",
    "                               'Support',\n",
    "                               'Confidence',\n",
    "                               'Lift',\n",
    "                               'Count'))\n",
    "    \n",
    "    Support =[]\n",
    "    Confidence = []\n",
    "    Lift = []\n",
    "    Items = []\n",
    "    Antecedent = []\n",
    "    Consequent=[]\n",
    "    Count = []\n",
    "    tfidf = []\n",
    "      \n",
    "    maxCount = 0\n",
    "    tfidfAverage = 0\n",
    "    \n",
    "    for RelationRecord in results:\n",
    "        for ordered_stat in RelationRecord.ordered_statistics:\n",
    "            #print(\"count:\", len(ordered_stat.items_base) + len(ordered_stat.items_add))\n",
    "            #print(\"cons:\", ordered_stat.items_add)\n",
    "            if IsAntecedentDifferentFromConsequent(ordered_stat.items_base, ordered_stat.items_add):\n",
    "                if maxCount < len(ordered_stat.items_base) + len(ordered_stat.items_add):\n",
    "                    maxCount = len(ordered_stat.items_base) + len(ordered_stat.items_add)\n",
    "                    \n",
    "                consequences = list(ordered_stat.items_base)\n",
    "                antecedent = list(ordered_stat.items_add)\n",
    "                Support.append(RelationRecord.support)\n",
    "                Antecedent.append(ordered_stat.items_base)\n",
    "                Consequent.append(ordered_stat.items_add)\n",
    "                Confidence.append(ordered_stat.confidence)\n",
    "                Lift.append(ordered_stat.lift)\n",
    "                Count.append(len(ordered_stat.items_base) + len(ordered_stat.items_add))\n",
    "                tfidfsum = 0\n",
    "                for c in consequences:\n",
    "                    tfidfsum += GetWordTFIDFMeasure(c, docsAsString)\n",
    "                    \n",
    "                for a in antecedent:\n",
    "                    tfidfsum += GetWordTFIDFMeasure(a, docsAsString)\n",
    "                \n",
    "                tfidfAverage =  tfidfsum/maxCount\n",
    "                \n",
    "    df['Left Hand Side'] = list(map(set, Antecedent))\n",
    "    df['Right Hand Side'] = list(map(set, Consequent))\n",
    "    df['Support'] = Support\n",
    "    df['Confidence'] = Confidence\n",
    "    df['Lift'] = Lift\n",
    "    df['Count'] = Count\n",
    "    df['TF-IDF Average'] = tfidfAverage\n",
    "    \n",
    "    df.sort_values(by ='Lift', ascending = False, inplace = True)\n",
    "            \n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "def SameStem(word1, word2, printStemmedWords = False):\n",
    "    lancaster = LancasterStemmer()\n",
    "    if printStemmedWords:\n",
    "        print(lancaster.stem(word1), lancaster.stem(word2))\n",
    "\n",
    "    return lancaster.stem(word1) == lancaster.stem(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsAntecedentDifferentFromConsequent(Antecedent, Consequent):\n",
    "    for ant in Antecedent:\n",
    "        for cons in Consequent:\n",
    "            if SameStem(ant, cons):\n",
    "                return False\n",
    "        \n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the ngrams and set the ngram thresholds\n",
    "min_count – Ignore all words and bigrams with total collected count lower than this value.\n",
    "threshold – Represent a score threshold for forming the phrases. A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold. Heavily depends on concrete scoring-function, see the scoring parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.phrases import Phrases,Phraser\n",
    "\n",
    "def ngrams(words, minimumCount=5, threshold=15):\n",
    "    bigram = Phrases(words,\n",
    "                     min_count=minimumCount,\n",
    "                     threshold=threshold)\n",
    "    \n",
    "    trigram = Phrases(bigram[words],\n",
    "                      threshold=threshold)  \n",
    "\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    trigram_mod = Phraser(trigram)\n",
    "    return bigram_mod, trigram_mod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF WORD CHECK\n",
    "Gets the TF-IDF value for a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import numpy as np\n",
    "\n",
    "def GetWordTFIDFMeasure(wordToFind, docsAsString):\n",
    "\n",
    "    tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docsAsString)\n",
    "\n",
    "    tfidf = tfidf_vectorizer_vectors.todense()\n",
    "\n",
    "    tfidf[tfidf == 0] = np.nan\n",
    "\n",
    "    means = np.nanmean(tfidf, axis=0)\n",
    "\n",
    "    means = dict(zip(tfidf_vectorizer.get_feature_names(), means.tolist()[0]))\n",
    "\n",
    "    tfidf = tfidf_vectorizer_vectors.todense()\n",
    "\n",
    "    ordered = np.argsort(tfidf*-1)\n",
    "    words = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    for i, doc in enumerate(docsAsString):\n",
    "        result = { }\n",
    "        for t in range(len(doc)):\n",
    "            if(words[ordered[i,t]] == wordToFind):\n",
    "                return means[words[ordered[i,t]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMEANS CLUSTERING\n",
    "https://www.kaggle.com/jbencina/clustering-documents-with-tfidf-and-kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def GetTopicsUsingTFIDFAndKMeansClustering(data, numberOfClusters, topNumberOfTerms):\n",
    "    tfidf = TfidfVectorizer(use_idf=True)\n",
    "    tfidf.fit(docsAsString)\n",
    "    text = tfidf.transform(docsAsString)\n",
    "    clusters = MiniBatchKMeans(n_clusters=10).fit_predict(text)\n",
    "    \n",
    "    df = pd.DataFrame(text.todense()).groupby(clusters).mean()\n",
    "\n",
    "    labels = tfidf.get_feature_names()\n",
    "    \n",
    "    results = []\n",
    "    for i,r in df.iterrows():\n",
    "        topic = [labels[t] for t in np.argsort(r)[-topNumberOfTerms:]]\n",
    "        \n",
    "        #results.append(','.join([labels[t] for t in np.argsort(r)[-topNumberOfTerms:]]))\n",
    "    \n",
    "        results.append(topic)\n",
    "    \n",
    "    return results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the qualifying articles from the metadata\n",
    "1. Qualifies if the publication date is within a given date range \n",
    "2. Has a pmc jcon file associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "def GetData(startDate, endDate, path):\n",
    "    listofarticles = []\n",
    "    try:\n",
    "        with open(path + 'metadata.csv') as f_in:\n",
    "            reader = csv.DictReader(f_in)\n",
    "            for row in reader:\n",
    "                if '-' not in row['publish_time']:\n",
    "                    continue\n",
    "                elif startDate > datetime.datetime.strptime(row['publish_time'], '%Y-%m-%d').date() or datetime.datetime.strptime(row['publish_time'], '%Y-%m-%d').date() > endDate:\n",
    "                    continue\n",
    "                if not row['pmc_json_files']:\n",
    "                    continue \n",
    "            \n",
    "                for json_path in row['pmc_json_files'].split(';'):\n",
    "                    listofarticles.append(article(row['publish_time'], json_path))\n",
    "                \n",
    "    except ValueError:\n",
    "        print(\"An error occurred: \", ValueError, \" Please try again.\")\n",
    "    return listofarticles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the textbodies from the list of pdfs\n",
    "1. Remove all special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def GetTextBodies(listOfpdfs, path):    \n",
    "    text = []    \n",
    "    \n",
    "    medicalWords = GetMedicalTerms(\"vocab.txt\")\n",
    "\n",
    "    for pdf in listOfpdfs:\n",
    "        filePath = path + pdf.Information.replace(\" \", \"\")\n",
    "\n",
    "        if not os.path.exists(filePath):\n",
    "            continue\n",
    "        \n",
    "        with open(filePath) as f_json:\n",
    "            full_text_dict = json.load(f_json)\n",
    "            textBody = []\n",
    "            for paragraph_dict in full_text_dict['body_text']:          \n",
    "                paragraph_text = re.sub(r'[^a-zA-Z_\\s]+', '', paragraph_dict['text'])   \n",
    "                paragraph_text = paragraph_text.lower()\n",
    "                \n",
    "                pdf.Information = paragraph_text\n",
    "    return listOfpdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the data pre-processing\n",
    "1. remove stop words\n",
    "2. lower case all words\n",
    "3. Check to see if the word is within the list of medical terms given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize  \n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "def CleanTheData(listOfDocs): \n",
    "    porter = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "  \n",
    "    resultDocs = []\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    medicalWords = GetMedicalTerms(\"vocab.txt\")\n",
    "    medicalStopWords = GetMedicalTerms(\"clinical-stopwords.txt\")\n",
    "    \n",
    "    for doc in listOfDocs:\n",
    "        result = []\n",
    "        for word in doc.Information.split(' '):  \n",
    "            \n",
    "            if word in \"\" or len(word) <= 3 or word in stop_words or word not in medicalWords or word in medicalStopWords:\n",
    "                continue\n",
    "                \n",
    "            result.append(word)\n",
    "                \n",
    "        doc.Information = result\n",
    "    return listOfDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the corpus\n",
    "1. Create the dictionary with all words and word ids\n",
    "2. Create the bi,tri, and quadgrams if applicable\n",
    "3. remove extreme occurences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "def ConvertDataToCorpus(cleaned_data):\n",
    "\n",
    "    dictionary = Dictionary(cleaned_data)\n",
    "    bigramMod, trigramMod = ngrams(cleaned_data)\n",
    "    \n",
    "    ngram =  [trigramMod[bigramMod[review]] for review in cleaned_data]\n",
    "       \n",
    "    id2word = gensim.corpora.Dictionary(ngram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.90, keep_tokens=['covid', 'coronavirus','sarscov'])\n",
    "    \n",
    "    id2word.compactify()\n",
    "    \n",
    "    corpus = [id2word.doc2bow(text) for text in ngram]\n",
    "    \n",
    "    return corpus, id2word, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all the topics from the dynamic topic model\n",
    "1. Print the top 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllTopicsFromModel(dtm):\n",
    "    topics = dtm.print_topics(3)\n",
    "    #print(topics)\n",
    "    \n",
    "    for topic in topics:\n",
    "        print(\"word:\", topic[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all qualifying topics generated from LDA model\n",
    "1. Get all topics from the LDA model\n",
    "2. Per each topic, check to see if the coherence score is greater than the lower bounce given\n",
    "3. Per word, check to see if the word or a related word exists in the current topic, if it does, do not add\n",
    "4. Per word, check to see if the probablity that the word is in the current topic is greater than the lower bound given\n",
    "5. Do not consider topics with only one qualifying word\n",
    "\n",
    "Notes: can read about coherence scores here: http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
    "https://stackoverflow.com/questions/54762690/coherence-score-0-4-is-good-or-bad \n",
    "https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def GetInformation(minProbablity, startDate, endDate, path):\n",
    "    \n",
    "    print('Starting', datetime.datetime.now())\n",
    "    listOfpdfs= GetData(startDate, endDate,path)\n",
    "    \n",
    "    if len(listOfpdfs) == 0:\n",
    "        print(\"No PDFs found under this topic\")\n",
    "        exit\n",
    "        \n",
    "    print(len(listOfpdfs))\n",
    "    \n",
    "    print('Finished gathering data', datetime.datetime.now())\n",
    "    data = GetTextBodies(listOfpdfs, path)\n",
    "    data.sort(key=lambda x: x.Date, reverse=False)\n",
    "    print('Finished sorting data', datetime.datetime.now())\n",
    "    cleaned_data = CleanTheData(data)  \n",
    "    print('Finished cleaning data', datetime.datetime.now())  \n",
    "\n",
    "    justDoc = []\n",
    "    for datum in cleaned_data:\n",
    "        justDoc.append(datum.Information)\n",
    "        \n",
    "        \n",
    "    \n",
    "    corpus, id2word, dictionary = ConvertDataToCorpus(justDoc) \n",
    "    print('Finished converting data to corpus', datetime.datetime.now())  \n",
    "    \n",
    "    dtm = GetTopicModel(corpus,\n",
    "                      id2word, \n",
    "                      dictionary,\n",
    "                      numberOfTopics = 50,\n",
    "                      chunkSize=2000)  \n",
    "    print('Finished creating LDA model', datetime.datetime.now())  \n",
    "    \n",
    "    GetAllTopicsFromModel(dtm)\n",
    "    \n",
    "#    topics = GetTopicTerms(searchTerm,\n",
    "#                           lda = lda, \n",
    "#                           id2word = id2word,\n",
    "#                           cleaned_data=cleaned_data, \n",
    "#                           lowerEndCoherenceScore=0, \n",
    "#                           numberOfWords=30, \n",
    "#                           minimumprobablity= minProbablity,\n",
    "#                           windowSize= 15,\n",
    "#                           processes=10)\n",
    "    \n",
    "    print('Finished creating DTM model', datetime.datetime.now())  \n",
    "    \n",
    " #   PrintRuleAssociation(lda, \n",
    " #                        support=0.1,\n",
    " #                        confidence=0.8,\n",
    " #                        bow=topics, \n",
    " #                        lift = 1, \n",
    " #                        length = None)\n",
    " #   print('Finished', datetime.datetime.now()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def PrintTopicsInTimeSlice(index, dtm, docsAsString):\n",
    "    dtmResults = dtm.print_topics(index,top_terms=10)  \n",
    "\n",
    "    dtmJustTheWords = []\n",
    "\n",
    "    for r in dtmResults:\n",
    "        words = {}\n",
    "        av = 0\n",
    "        for word in r:\n",
    "            #print(word)\n",
    "            if \"_\" in word[0]:\n",
    "                continue\n",
    "            \n",
    "            tfidf = GetWordTFIDFMeasure(word[0], docsAsString)\n",
    "            print(word[0], tfidf)\n",
    "            if tfidf is None:\n",
    "                continue\n",
    "            av += tfidf\n",
    "        print('average TF-IDF: ', av/10)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRulesFromTFIDFClusters(start, end, cleaned_data):\n",
    "\n",
    "    docsAsString = []\n",
    "\n",
    "    for datum in cleaned_data:\n",
    "        if GetArticlesFromDateRange(start,end, datum.Date):\n",
    "            docsAsString.append(' '.join(str(info) for info in datum.Information))\n",
    "                \n",
    "    t = GetTopicsUsingTFIDFAndKMeansClustering(docsAsString, 20,10)\n",
    "    PrintRuleAssociation(support=0.175,\n",
    "                         confidence=0.9,\n",
    "                         bow=t, \n",
    "                         listOfDocs = docsAsString,\n",
    "                         lift = 1, \n",
    "                         length = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#first month = 2330\n",
    "#second month = 5992\n",
    "#third month = 9234\n",
    "#fourth month = 9486\n",
    "#fifth =  8986\n",
    "#sixth = 8470\n",
    "#7th = 9178\n",
    "#8th = 9325\n",
    "#9th = 8368\n",
    "#10th = 7284\n",
    "#11th = 8857\n",
    "#12th = 5059\n",
    "#Finished gathering data 2021-03-16 21:17:36.279353\n",
    "\n",
    "#Finished gathering data 2021-03-16 21:17:52.412954\n",
    "#Finished gathering data 2021-03-16 21:20:53.203475\n",
    "\n",
    "\n",
    "with stop word filter:\n",
    "Finished gathering data 2021-03-16 21:55:08.355088\n",
    "88349\n",
    "Finished gathering data 2021-03-16 21:55:24.637484\n",
    "Finished gathering data 2021-03-16 21:58:17.915549\n",
    "Finished cleaning data 2021-03-16 22:31:32.614923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2021-04-15 06:37:09.631233\n",
      "17045\n",
      "Finished gathering data 2021-04-15 06:37:25.662275\n"
     ]
    }
   ],
   "source": [
    "startDate = datetime.date(2020, 3, 1)\n",
    "endDate = datetime.date(2020, 6, 1)\n",
    "path = '/Users/josekowsky/Documents/2021-01-22/'\n",
    "\n",
    "print('Starting', datetime.datetime.now())\n",
    "listOfpdfs= GetData(startDate, endDate,path)\n",
    "    \n",
    "if len(listOfpdfs) == 0:\n",
    "    print(\"No PDFs found under this topic\")\n",
    "    exit\n",
    "        \n",
    "print(len(listOfpdfs))\n",
    "    \n",
    "print('Finished gathering data', datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished sorting data 2021-04-15 06:37:25.669052\n",
      "Finished sorting data 2021-04-15 06:38:00.563366\n"
     ]
    }
   ],
   "source": [
    "print('Finished sorting data', datetime.datetime.now())\n",
    "data = GetTextBodies(listOfpdfs, path)\n",
    "data.sort(key=lambda x: x.Date, reverse=True)\n",
    "print('Finished sorting data', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning data 2021-04-15 06:38:00.570723\n"
     ]
    }
   ],
   "source": [
    "print('Finished cleaning data', datetime.datetime.now()) \n",
    "cleaned_data = CleanTheData(data)  \n",
    "print('Finished cleaning data', datetime.datetime.now())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Finished converting data to corpus', datetime.datetime.now())  \n",
    "justDoc = []\n",
    "docsAsString = []\n",
    "for datum in cleaned_data:\n",
    "    justDoc.append(datum.Information)\n",
    "    \n",
    "    \n",
    "    docsAsString.append(' '.join(datum.Information))\n",
    "          \n",
    "    \n",
    "corpus, id2word, dictionary = ConvertDataToCorpus(justDoc) \n",
    "print('Finished converting data to corpus', datetime.datetime.now())  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Finished DTM', datetime.datetime.now())  \n",
    "dtm = GetTopicModel(corpus,\n",
    "                    id2word, \n",
    "                    dictionary,\n",
    "                    numberOfTopics = 5,\n",
    "                    chunkSize=2000)  \n",
    "\n",
    "print('Finished DTM', datetime.datetime.now())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintTopicsInTimeSlice(0,dtm, docsAsString)\n",
    "print('Finished DTM 0', datetime.datetime.now()) \n",
    "PrintTopicsInTimeSlice(1,dtm, docsAsString)\n",
    "print('Finished DTM 1', datetime.datetime.now()) \n",
    "PrintTopicsInTimeSlice(2,dtm, docsAsString)\n",
    "print('Finished DTM 2', datetime.datetime.now()) \n",
    "\n",
    "startDate = datetime.date(2020, 3, 1)\n",
    "endDate = datetime.date(2020, 4, 1)\n",
    "GetRulesFromTFIDFClusters(start, end, cleaned_data)\n",
    "print('Finished TFIDF 0', datetime.datetime.now()) \n",
    "\n",
    "startDate = datetime.date(2020, 4, 1)\n",
    "endDate = datetime.date(2020, 5, 1)\n",
    "GetRulesFromTFIDFClusters(start, end, cleaned_data)\n",
    "print('Finished TFIDF 1', datetime.datetime.now()) \n",
    "    \n",
    "startDate = datetime.date(2020, 5, 1)\n",
    "endDate = datetime.date(2020, 6, 1)\n",
    "GetRulesFromTFIDFClusters(start, end, cleaned_data)\n",
    "print('Finished TFIDF 2', datetime.datetime.now()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
