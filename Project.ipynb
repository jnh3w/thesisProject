{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I had to install nltk to get this project to work. I might have downloaded something for\n",
    "#I think I used this in jupyter notebook: nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "from apyori import apriori\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from datetime import datetime\n",
    "\n",
    "def PrintRuleAssociation(word, maxNumberOfAssociations = 2):\n",
    "    bow = []\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    severe = \"severe\"\n",
    "    severe = lemmatizer.lemmatize(severe)\n",
    "    severe = porter.stem(severe)\n",
    "    \n",
    "    covid = \"covid\"\n",
    "    covid = lemmatizer.lemmatize(covid)\n",
    "    covid = porter.stem(covid)\n",
    "    \n",
    "    sarscov = \"sarscov\"\n",
    "    \n",
    "    sarscov = lemmatizer.lemmatize(sarscov)\n",
    "    sarscov = porter.stem(sarscov)\n",
    "      \n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    word = porter.stem(word)\n",
    "    \n",
    "    infect= \"infect\"\n",
    "    infect = lemmatizer.lemmatize(infect)\n",
    "    infect = porter.stem(infect)\n",
    "    \n",
    "    bow = GetTopicTerms(lda, word) \n",
    "    \n",
    "    if len(bow) == 0:\n",
    "        print('No topics found')\n",
    "        return\n",
    "    \n",
    "    print('amount of topics for ', word, \": \", len(bow))\n",
    "    rules = apriori(bow, min_support = 0.01, min_confidence = 0.7, max_length = maxNumberOfAssociations)\n",
    "    results = list(rules)\n",
    "    \n",
    "    df = pd.DataFrame(columns=('Left Hand Side','Right Hand Side','Support','Confidence','Lift'))\n",
    "    \n",
    "    Support =[]\n",
    "    Confidence = []\n",
    "    Lift = []\n",
    "    Items = []\n",
    "    Antecedent = []\n",
    "    Consequent=[]\n",
    "      \n",
    "    for RelationRecord in results:\n",
    "        for ordered_stat in RelationRecord.ordered_statistics:\n",
    "            consequences = list(ordered_stat.items_base)\n",
    "            antecedent = list(ordered_stat.items_add)\n",
    "            if ((covid in consequences or covid in antecedent) or (sarscov in consequences or sarscov in antecedent) or (infect in consequences or infect in antecedent)) \\\n",
    "            and (word in consequences or word in antecedent) \\\n",
    "            and (severe in consequences or severe in antecedent):\n",
    "                Support.append(RelationRecord.support)\n",
    "                Antecedent.append(ordered_stat.items_base)\n",
    "                Consequent.append(ordered_stat.items_add)\n",
    "                Confidence.append(ordered_stat.confidence)\n",
    "                Lift.append(ordered_stat.lift)\n",
    "\n",
    "                                          \n",
    "    df['Left Hand Side'] = list(map(set, Antecedent))\n",
    "    df['Right Hand Side'] = list(map(set, Consequent))\n",
    "    df['Support'] = Support\n",
    "    df['Confidence'] = Confidence\n",
    "    df['Lift'] = Lift\n",
    "    df.sort_values(by ='Lift', ascending = False, inplace = True)\n",
    "    print(df)\n",
    "    #display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "def ngrams(words, minimumCount=5, threshold=100):\n",
    "    bigram = gensim.models.Phrases(words, min_count=minimumCount, threshold=threshold) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[words], threshold=threshold)  \n",
    "\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    return bigram_mod, trigram_mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HasPhraseInAbstract(phrases, textToCompare):\n",
    "    for phrase in phrases:\n",
    "        if phrase in textToCompare:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def GetData():\n",
    "    data = defaultdict(list)   \n",
    "    listofarticles = []\n",
    "    with open('metadata.csv') as f_in:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        for row in reader:\n",
    "            abstract = row['abstract'].lower()\n",
    "            if '-' not in row['publish_time']:\n",
    "                continue\n",
    "            elif datetime.strptime(row['publish_time'], '%Y-%m-%d').month != datetime(2020, 3, 1).month:\n",
    "                continue\n",
    "            if not row['pdf_json_files']:\n",
    "                continue   \n",
    "            \n",
    "            for json_path in row['pdf_json_files'].split(';'):\n",
    "                json_file = Path(json_path)\n",
    "                if json_file.is_file():\n",
    "                    listofarticles.append(json_path)\n",
    "                    data[row['cord_uid']] = True\n",
    "                \n",
    "    return listofarticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def GetTextBodies(listOfpdfs):    \n",
    "    text = []    \n",
    "    for json_path in listOfpdfs:\n",
    "        with open(json_path.replace(\" \", \"\")) as f_json:\n",
    "            full_text_dict = json.load(f_json)\n",
    "            textBody = []\n",
    "            for paragraph_dict in full_text_dict['body_text']:    \n",
    "                paragraph_text = paragraph_dict['text'].replace('sars-cov-19', 'covid')          \n",
    "                paragraph_text = re.sub(r'[^a-zA-Z_\\s]+', '', paragraph_text)             \n",
    "                textBody.append(paragraph_text)\n",
    "        text.append(paragraph_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceAntonymsForSevere(word):\n",
    "    wordsForSever = [\"severe\", \"critical\",\"hospitalization\",\"death\",\"died\",\"dead\",\n",
    "                     \"cytokine storm\",\"serious\",\"icu\",\"critical care\",\"acute\",\"grave\",\n",
    "                     \"dire\",\"bleak\",\"mortality\",\"risk\"]\n",
    "    if word in wordsForSever:\n",
    "        return \"severe\"\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize  \n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "def RemoveStopWords(listOfDocs): \n",
    "    porter = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    stop_words.extend(['correspond','author','submiss','includ','present','studi','editori','contribut','author',\n",
    "                       'sole','email','financi','info','author','declar','follow','payment','servic','publicli',\n",
    "                       'avail','activ','express','involv','relationship','twitter','multivari','analysi','journal',\n",
    "                       'accuraci','obtain','train','perform','appli','preprocess','intern','optim','sequenc',\n",
    "                       'posit','specif','use','version','manuscript','read','agre','publish','wrote','review',\n",
    "                       'literatur','final','version','draft','read','approv','increas','mgkg','intraven','ivermectin',\n",
    "                       'mycoplasma','ovi','administ','metaanalysi','meta','analysi','find','live','peer','preval',\n",
    "                       'return','meaning','life','engag','hmitoxantron','hmitoxantron','hmitoxantron','mbamb','hepg',\n",
    "                       'mcfa','note','also','street','imper','parp','people','pandemic','student','ttest','lipid',\n",
    "                       'accord','anime','committe','research','univers','need','muscl','panel','shown','indic',\n",
    "                       'control','larger','central','subset','sampl','peerreview','copyright','holder','receiv',\n",
    "                       'extern','fund','spong','httpsdoiorg','upon','request','interpret','write','data','particip',\n",
    "                       'month','deidentifi','platform','nation','institut','japan','bayesian','written','inform',\n",
    "                       'consent','clinic','heparanas','american','umi','test','augment','preprint','post','juli',\n",
    "                       'medrxiv','acquisit','import','intellectu','content','substanti','concept','design','revis',\n",
    "                       'critic','workflow','system','consum','funder','age','year','restrict','cubic','spline',\n",
    "                       'incid','trend','biopsi','start','results','mirna','silico','earli','laboratori','compani',\n",
    "                       'monoclon','screen','target','valid','group','formal','methodolog','vaccin',\n",
    "                       'term','time','nodul','worden','wurd','delay','licens','display','tmprss','right','number',\n",
    "                       'social','perinat','csc','onlin','rappel','waterpip','work','office','total',\n",
    "                       'confin','chair','hour','ecog','complet','disabl','cannot','carri','selfcar','light',\n",
    "                       'sector','compet','interest','center','creativ','common','guidelin','properli',\n",
    "                       'cite','england','rest','patient','treatment','breast','technic','effect','conflict','disclos',\n",
    "                       'cell','cells','demograph','characterist','demonstr','pictur','boehring','ingelheim','angiotensinconvert',\n",
    "                       'enzym','june','current','child','january','feburary','march','april','may','august','september',\n",
    "                       'october','november','december','copi','licenc','visit','httpcreativecommonsorglicen',\n",
    "                       'complianc','icmj','uniform','disclosur','ethic','hajj','doctor','analyz','mmoll','tabl'])\n",
    "    \n",
    "    resultDocs = []\n",
    "    words = set(nltk.corpus.words.words())\n",
    "\n",
    "    for doc in listOfDocs:\n",
    "        result = []\n",
    "        for word in doc.split(' '):\n",
    "            lowerCasedWord = word.lower()\n",
    "            lemmedWord = lemmatizer.lemmatize(ReplaceAntonymsForSevere(lowerCasedWord))\n",
    "                          \n",
    "            if lemmedWord not in stop_words and lemmedWord not in \"\" and len(lemmedWord) > 3:\n",
    "                stemmedWord = porter.stem(lemmedWord)\n",
    "                if stemmedWord not in stop_words:\n",
    "                    result.append(porter.stem(lemmedWord))\n",
    "        resultDocs.append(result)\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "def ConvertDataToCorpus(cleaned_data):\n",
    "    dictionary = Dictionary(cleaned_data)\n",
    "    bigramMod, trigramMod = ngrams(cleaned_data)\n",
    "    \n",
    "    ngram =  [trigramMod[bigramMod[review]] for review in cleaned_data]\n",
    "    \n",
    "    id2word = gensim.corpora.Dictionary(ngram)\n",
    "    id2word.compactify()\n",
    "    \n",
    "    corpus = [id2word.doc2bow(text) for text in ngram]\n",
    "    \n",
    "    return corpus, id2word, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLDAModel(corpus, id2word,dictionary, numberOfTopics = 10, chunkSize=2000, passes=10):\n",
    "\n",
    "    temp = dictionary[0]  \n",
    "\n",
    "    return gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                   num_topics=numberOfTopics,\n",
    "                                                   id2word=id2word,\n",
    "                                                   chunksize=chunkSize,\n",
    "                                                   workers=5, # Num. Processing Cores - 1\n",
    "                                                   passes=passes,\n",
    "                                                   eval_every = 1,\n",
    "                                                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTopics(word):\n",
    "    for key, value in id2word.items(): \n",
    "        porter = PorterStemmer()\n",
    "        term = porter.stem(word)\n",
    "        \n",
    "        if value == term: \n",
    "            #print(key, value)\n",
    "            for topic in lda.get_term_topics(key):\n",
    "                #print(topic)\n",
    "            \n",
    "                for wordId in lda.get_topic_terms(topic[0]):\n",
    "                    #print(wordId)\n",
    "                    print(' ,',id2word[wordId[0]])\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTopicTerms(lda, word):\n",
    "    bow = []\n",
    "    for key, value in id2word.items(): \n",
    "        porter = PorterStemmer()\n",
    "        term = porter.stem(word)\n",
    "        \n",
    "        if value == term: \n",
    "            #print(key, value)\n",
    "            for topic in lda.get_term_topics(key):\n",
    "                #print(topic)\n",
    "                transactions = []\n",
    "                for wordId in lda.get_topic_terms(topic[0]):\n",
    "                    transactions.append(id2word[wordId[0]])\n",
    "                bow.append(transactions)   \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 21:12:35.343807\n",
      "Amount of pdfs gathered:  6476\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One of texts or corpus has to be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-83ade2630973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvertDataToCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetLDAModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumberOfTopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunkSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#PrintRuleAssociation(word, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mtop_topics\u001b[0;34m(self, corpus, texts, dictionary, window_size, coherence, topn, processes)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \"\"\"\n\u001b[0;32m-> 1265\u001b[0;31m         cm = CoherenceModel(\n\u001b[0m\u001b[1;32m   1266\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoherence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, topics, texts, corpus, dictionary, window_size, keyed_vectors, coherence, topn, processes)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyed_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyed_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeyed_vectors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"One of texts or corpus has to be provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# Check if associated dictionary is provided.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: One of texts or corpus has to be provided."
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "listOfpdfs= GetData()\n",
    "    \n",
    "if len(listOfpdfs) == 0:\n",
    "    print(\"No PDFs found under this topic\")\n",
    "    \n",
    "print('Amount of pdfs gathered: ', len(listOfpdfs))\n",
    "data = GetTextBodies(listOfpdfs)\n",
    "cleaned_data = RemoveStopWords(data)\n",
    "corpus, id2word, dictionary = ConvertDataToCorpus(cleaned_data)\n",
    "lda = GetLDAModel(corpus, id2word, dictionary, numberOfTopics = 40,chunkSize=2000)\n",
    "lda.show_topics()\n",
    "#PrintRuleAssociation(word, 3)\n",
    "    \n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(26,\n",
       "  '0.015*\"sever\" + 0.014*\"diseas\" + 0.011*\"result\" + 0.010*\"infect\" + 0.008*\"public_health\" + 0.008*\"provid\" + 0.007*\"covid\" + 0.007*\"approach\" + 0.006*\"conclus\" + 0.006*\"spread\"'),\n",
       " (9,\n",
       "  '0.005*\"materi\" + 0.004*\"infect\" + 0.004*\"process\" + 0.004*\"autopsi\" + 0.004*\"intub\" + 0.004*\"pellet\" + 0.004*\"respir\" + 0.004*\"para\" + 0.004*\"hcw\" + 0.003*\"region\"'),\n",
       " (11,\n",
       "  '0.017*\"model\" + 0.014*\"method\" + 0.009*\"result\" + 0.008*\"propos\" + 0.007*\"futur\" + 0.007*\"user\" + 0.006*\"protein\" + 0.005*\"show\" + 0.005*\"approach\" + 0.005*\"provid\"'),\n",
       " (14,\n",
       "  '0.026*\"infect\" + 0.021*\"sever\" + 0.013*\"diseas\" + 0.012*\"viru\" + 0.011*\"transmiss\" + 0.007*\"infecti\" + 0.007*\"covid\" + 0.007*\"pathogen\" + 0.007*\"respiratori\" + 0.005*\"provid\"'),\n",
       " (15,\n",
       "  '0.019*\"viru\" + 0.017*\"infect\" + 0.014*\"protein\" + 0.013*\"viral\" + 0.010*\"gene\" + 0.010*\"develop\" + 0.009*\"respons\" + 0.007*\"antivir\" + 0.007*\"bind\" + 0.007*\"sever\"'),\n",
       " (1,\n",
       "  '0.015*\"infect\" + 0.012*\"sever\" + 0.007*\"respons\" + 0.006*\"associ\" + 0.006*\"viru\" + 0.005*\"high\" + 0.005*\"figur\" + 0.005*\"result\" + 0.005*\"report\" + 0.005*\"gene\"'),\n",
       " (29,\n",
       "  '0.011*\"sever\" + 0.011*\"model\" + 0.010*\"health\" + 0.009*\"diseas\" + 0.009*\"covid\" + 0.008*\"care\" + 0.007*\"epidem\" + 0.007*\"develop\" + 0.005*\"case\" + 0.005*\"outbreak\"'),\n",
       " (35,\n",
       "  '0.009*\"sarscov\" + 0.006*\"dock\" + 0.005*\"high\" + 0.005*\"bind\" + 0.005*\"develop\" + 0.005*\"sever\" + 0.005*\"bioaerosol\" + 0.005*\"result\" + 0.005*\"site\" + 0.004*\"infect\"'),\n",
       " (30,\n",
       "  '0.007*\"provid\" + 0.005*\"health\" + 0.004*\"develop\" + 0.004*\"access\" + 0.004*\"link\" + 0.004*\"appropri_credit_origin_sourc\" + 0.004*\"adapt_distribut_reproduct\" + 0.004*\"medium_format_long_give\" + 0.004*\"result\" + 0.004*\"chang\"'),\n",
       " (20,\n",
       "  '0.024*\"none\" + 0.014*\"infect\" + 0.011*\"sever\" + 0.007*\"strain\" + 0.007*\"result\" + 0.007*\"asthma\" + 0.006*\"covid\" + 0.005*\"symptom\" + 0.005*\"viral\" + 0.005*\"care\"')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
