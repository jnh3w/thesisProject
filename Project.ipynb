{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from apyori import apriori\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from datetime import datetime\n",
    "\n",
    "def PrintRuleAssociation():\n",
    "    bow = []\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    bow = GetTopicTerms(lda, word) \n",
    "    \n",
    "    if len(bow) == 0:\n",
    "        print('No topics found')\n",
    "        return\n",
    "    \n",
    "    rules = apriori(bow, min_support = 0.2, min_confidence = 0.9)\n",
    "    results = list(rules)\n",
    "    \n",
    "    df = pd.DataFrame(columns=('Left Hand Side','Right Hand Side','Support','Confidence','Lift'))\n",
    "    \n",
    "    Support =[]\n",
    "    Confidence = []\n",
    "    Lift = []\n",
    "    Items = []\n",
    "    Antecedent = []\n",
    "    Consequent=[]\n",
    "      \n",
    "    for RelationRecord in results:\n",
    "        for ordered_stat in RelationRecord.ordered_statistics:\n",
    "            consequences = list(ordered_stat.items_base)\n",
    "            antecedent = list(ordered_stat.items_add)\n",
    "            if ((covid in consequences or covid in antecedent) or (sarscov in consequences or sarscov in antecedent) or (infect in consequences or infect in antecedent)) \\\n",
    "            and (word in consequences or word in antecedent) \\\n",
    "            and (severe in consequences or severe in antecedent):\n",
    "                Support.append(RelationRecord.support)\n",
    "                Antecedent.append(ordered_stat.items_base)\n",
    "                Consequent.append(ordered_stat.items_add)\n",
    "                Confidence.append(ordered_stat.confidence)\n",
    "                Lift.append(ordered_stat.lift)\n",
    "\n",
    "                                          \n",
    "    df['Left Hand Side'] = list(map(set, Antecedent))\n",
    "    df['Right Hand Side'] = list(map(set, Consequent))\n",
    "    df['Support'] = Support\n",
    "    df['Confidence'] = Confidence\n",
    "    df['Lift'] = Lift\n",
    "    df.sort_values(by ='Lift', ascending = False, inplace = True)\n",
    "    print(df)\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "def ngrams(words, minimumCount=5, threshold=100):\n",
    "    bigram = gensim.models.Phrases(words, min_count=minimumCount, threshold=threshold) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[words], threshold=threshold)  \n",
    "\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    return bigram_mod, trigram_mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-72-e2661e5f444c>, line 56)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-72-e2661e5f444c>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    values = r.get('values', [])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#needed to do this: pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
    "#this may have been a waste of time\n",
    "\n",
    "from __future__ import print_function\n",
    "import pickle\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import io\n",
    "\n",
    "# If modifying these scopes, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly']\n",
    "\n",
    "def GetDataFromGoogleDrive():\n",
    "    \"\"\"Shows basic usage of the Drive v3 API.\n",
    "    Prints the names and ids of the first 10 files the user has access to.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    #get metadata\n",
    "    # Call the Drive v3 API\n",
    "    results = service.files().list(\n",
    "        pageSize=10, fields=\"nextPageToken, files(id, name)\").execute()\n",
    "    items = results.get('files', [])\n",
    "\n",
    "    #print(type(items))\n",
    "    #document = service.documents().get(documentId=DOCUMENT_ID).execute()\n",
    "\n",
    "    #print('The title of the document is: {}'.format(document.get('title')))\n",
    "    #print(items)\n",
    "\n",
    "    file_id = '1dDxbn8_NcmihPQMeE7wzIuwRp0huPCpq'\n",
    "    \n",
    "    request = service.files().export_media(fileId=file_id,\n",
    "                                           sheet = service.spreadsheets(),\n",
    "                                           r = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "                                range=SAMPLE_RANGE_NAME).execute()\n",
    "    values = r.get('values', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Resource' object has no attribute 'spreadsheets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-2795c85f9731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetDataFromGoogleDrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-885b65c82f12>\u001b[0m in \u001b[0;36mGetDataFromGoogleDrive\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mfile_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1dDxbn8_NcmihPQMeE7wzIuwRp0huPCpq'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0msheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspreadsheets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     r = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID,\n\u001b[1;32m     54\u001b[0m                                 range=SAMPLE_RANGE_NAME).execute()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Resource' object has no attribute 'spreadsheets'"
     ]
    }
   ],
   "source": [
    "t = GetDataFromGoogleDrive()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "#change this to take a date time range and check for the pmc files because they have a cleaner textbody\n",
    "#we no longer need the abstract to filter out based on keywords\n",
    "def GetData(startDate, endDate):\n",
    "    data = defaultdict(list)   \n",
    "    listofarticles = []\n",
    "    try:\n",
    "        with open('metadata.csv') as f_in:\n",
    "            reader = csv.DictReader(f_in)\n",
    "            for row in reader:\n",
    "                if '-' not in row['publish_time']:\n",
    "                    continue\n",
    "                elif not (startDate <= datetime.strptime(row['publish_time'], '%Y-%m-%d') <= endDate):\n",
    "                    continue\n",
    "                if not row['pmc_json_files']:\n",
    "                    continue   \n",
    "            \n",
    "                for json_path in row['pmc_json_files'].split(';'):\n",
    "                    json_file = Path(json_path)\n",
    "                    if json_file.is_file():\n",
    "                        listofarticles.append(json_path)\n",
    "                        data[row['cord_uid']] = True\n",
    "                \n",
    "    except ValueError:\n",
    "        print(\"An error occurred: \", ValueError, \" Please try again.\")\n",
    "    return listofarticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def GetTextBodies(listOfpdfs):    \n",
    "    text = []    \n",
    "    for json_path in listOfpdfs:\n",
    "        with open(json_path.replace(\" \", \"\")) as f_json:\n",
    "            full_text_dict = json.load(f_json)\n",
    "            textBody = []\n",
    "            for paragraph_dict in full_text_dict['body_text']:          \n",
    "                paragraph_text = re.sub(r'[^a-zA-Z_\\s]+', '', paragraph_dict['text'])             \n",
    "                textBody.append(paragraph_text)\n",
    "        text.append(paragraph_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize  \n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "def RemoveStopWords(listOfDocs): \n",
    "    porter = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    resultDocs = []\n",
    "    words = set(nltk.corpus.words.words())\n",
    "\n",
    "    for doc in listOfDocs:\n",
    "        result = []\n",
    "        for word in doc.split(' '):\n",
    "            lowerCasedWord = word.lower()\n",
    "            lemmedWord = lemmatizer.lemmatize(lowerCasedWord)\n",
    "                          \n",
    "            if lemmedWord not in stop_words and lemmedWord not in \"\" and len(lemmedWord) > 3:\n",
    "                stemmedWord = porter.stem(lemmedWord)\n",
    "                if stemmedWord not in stop_words:\n",
    "                    result.append(porter.stem(lemmedWord))\n",
    "        resultDocs.append(result)\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "def ConvertDataToCorpus(cleaned_data):\n",
    "    dictionary = Dictionary(cleaned_data)\n",
    "    bigramMod, trigramMod = ngrams(cleaned_data)\n",
    "    \n",
    "    ngram =  [trigramMod[bigramMod[review]] for review in cleaned_data]\n",
    "    \n",
    "    id2word = gensim.corpora.Dictionary(ngram)\n",
    "    id2word.compactify()\n",
    "    \n",
    "    corpus = [id2word.doc2bow(text) for text in ngram]\n",
    "    \n",
    "    return corpus, id2word, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-evaluate the parameters here, mess with the hyper parameter\n",
    "def GetLDAModel(corpus, id2word,dictionary, numberOfTopics = 10, chunkSize=2000, passes=10):\n",
    "\n",
    "    temp = dictionary[0]  \n",
    "\n",
    "    return gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                   num_topics=numberOfTopics,\n",
    "                                                   id2word=id2word,\n",
    "                                                   chunksize=chunkSize,\n",
    "                                                   workers=5, # Num. Processing Cores - 1\n",
    "                                                   passes=passes,\n",
    "                                                   eval_every = 1,\n",
    "                                                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to get only top topics\n",
    "def GetTopicTerms(lda):\n",
    "    bow = []\n",
    "\n",
    "    for topic in lda.show_topic():\n",
    "        print(topic)\n",
    "        bow.append(list(map(lambda x: x[0], topic)))   \n",
    "        \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add functionality to read from the google docs api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-17 09:59:09.698087\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'metadata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-dca7847c1cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlistOfpdfs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mGetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistOfpdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-80c97ae654f2>\u001b[0m in \u001b[0;36mGetData\u001b[0;34m(startDate, endDate)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlistofarticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'metadata.csv'"
     ]
    }
   ],
   "source": [
    "#split this up in a way that's easier to test, try splitting it up just a month at a time, find a way to visual topics\n",
    "#per slice\n",
    "\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "listOfpdfs= GetData(datetime.date(2020, 3, 1), datetime.date(2020, 3, 15))\n",
    "    \n",
    "if len(listOfpdfs) == 0:\n",
    "    print(\"No PDFs found under this topic\")\n",
    "    \n",
    "print('Amount of pdfs gathered: ', len(listOfpdfs))\n",
    "data = GetTextBodies(listOfpdfs)\n",
    "cleaned_data = RemoveStopWords(data)\n",
    "corpus, id2word, dictionary = ConvertDataToCorpus(cleaned_data)\n",
    "lda = GetLDAModel(corpus, id2word, dictionary, numberOfTopics = 40,chunkSize=2000)\n",
    "lda.show_topics()\n",
    "#PrintRuleAssociation(word, 3)\n",
    "    \n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
