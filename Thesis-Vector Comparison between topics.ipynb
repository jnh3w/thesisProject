{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871297f6",
   "metadata": {},
   "source": [
    "Tests for this notebook:\n",
    "\n",
    "1. Check to see if the keywords in each document correspond with a certain topic\n",
    "2. Check to see if there's a difference between TF-IDF and BERT with this keyword extraction\n",
    "3. Check to see if there's a difference using the different topic modeling\n",
    "\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/07/topic-modeling-with-naive-bayes-classifier/\n",
    "\n",
    "\n",
    "TODO tomorrow - Do the Kmeans first and elbow test and use that k value to determine lda topics and then compare documents in clusters\n",
    "\n",
    "\n",
    "TODO:\n",
    "\n",
    "1. Add Ngram\n",
    "2. Add back data cleaning and stemming/Lemming\n",
    "3. check the data and cosine simularity before 1 and 2 though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd3a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(topic_1, topic_2):\n",
    "    \"\"\"\n",
    "    Derives the Jaccard similarity of two topics\n",
    "\n",
    "    Jaccard similarity:\n",
    "    - A statistic used for comparing the similarity and diversity of sample sets\n",
    "    - J(A,B) = (A ∩ B)/(A ∪ B)\n",
    "    - Goal is low Jaccard scores for coverage of the diverse elements\n",
    "    \"\"\"\n",
    "    intersection = set(topic_1).intersection(set(topic_2))\n",
    "    union = set(topic_1).union(set(topic_2))\n",
    "                    \n",
    "    return float(len(intersection))/float(len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d26775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "def GetOptimalKMeans(X, maxK):\n",
    "    model = KMeans(n_init=1000,random_state=3425, max_iter=3000, tol=0.001)\n",
    "    visualizer = KElbowVisualizer(model, k=(2,maxK), metric='distortion', timings=False, locate_elbow=True)\n",
    "\n",
    "    visualizer.fit(X)        # Fit the data to the visualizer\n",
    "    visualizer.show()        # Finalize and render the figure\n",
    "    \n",
    "    print(\"Optimal K-value:\", visualizer.elbow_value_)\n",
    "    \n",
    "#     visualizer = KElbowVisualizer(model, k=(2,maxK), metric='silhouette', timings=False)\n",
    "\n",
    "#     visualizer.fit(X)        # Fit the data to the visualizer\n",
    "#     visualizer.show()        # Finalize and render the figure\n",
    "    \n",
    "#     print(\"Optimal K-value:\", visualizer.elbow_value_)\n",
    "    return visualizer.elbow_value_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385f9198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92bce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os.path\n",
    "from langdetect import detect\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def GetData(startDate, endDate, path):\n",
    "    listofarticles = []\n",
    "    try:\n",
    "        with open(path + 'metadata.csv') as f_in:\n",
    "            reader = csv.DictReader(f_in)\n",
    "            for row in reader:\n",
    "                if '-' not in row['publish_time']:\n",
    "                    continue\n",
    "                elif startDate > datetime.datetime.strptime(row['publish_time'], '%Y-%m-%d').date() or datetime.datetime.strptime(row['publish_time'], '%Y-%m-%d').date() > endDate:\n",
    "                    continue\n",
    "                if not row['abstract']:\n",
    "                    continue \n",
    "                      \n",
    "                try:\n",
    "                    language = detect(row['abstract'])\n",
    "                    if 'en' != language:\n",
    "                        continue\n",
    "                except:\n",
    "                    continue\n",
    "                filePath = path + row['pmc_json_files']\n",
    "#                 if( not os.path.isfile(filePath)):\n",
    "#                     continue\n",
    "#                 with open(filePath) as ff_in:\n",
    "#                     reader = csv.DictReader(f_in)\n",
    "#                     full_text_dict = json.load(f_json\n",
    "#                     print(reader)\n",
    "                                                                                    \n",
    "                introduction = ''    \n",
    "                if row['pmc_json_files']:\n",
    "                    for json_path in row['pmc_json_files'].split('; '):\n",
    "                        with open(path + json_path) as f_json:\n",
    "                            full_text_dict = json.load(f_json)\n",
    "\n",
    "                            for paragraph_dict in full_text_dict['body_text']:\n",
    "                                paragraph_text = paragraph_dict['text']\n",
    "                                section_name = paragraph_dict['section']\n",
    "                                introduction += paragraph_text\n",
    "                if '\\\\' in introduction or introduction.strip() == \"\" or len(introduction.split()) < 50:\n",
    "                    continue\n",
    "                               \n",
    "                listofarticles.append(re.sub(r'[^A-Za-z ]+', '', introduction.lower()))\n",
    "                \n",
    "    except ValueError:\n",
    "        print(\"An error occurred: \", ValueError, \" Please try again.\")\n",
    "    return listofarticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b05f63da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True) # initialize(36) or initialize(os.cpu_count()-1)\n",
    "\n",
    "def tokenize_lemma_stopwords(text):\n",
    "    tokens = []\n",
    "    for token in nltk.tokenize.word_tokenize(text.lower()):\n",
    "#         if(len(token) <= 3 and token in stopwords.words()):\n",
    "#             continue\n",
    "        if(len(token) <= 3 or token in stemmedStopWords):\n",
    "            continue\n",
    "        tokens.append(ps.stem(token))\n",
    "            \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def dataCleaning(data):\n",
    "    data[\"content\"] = data[\"content\"].parallel_apply(tokenize_lemma_stopwords)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6690fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.phrases import Phrases,Phraser\n",
    "\n",
    "def ngrams(words, minimumCount=5, threshold=15):\n",
    "    bigram = Phrases(words,\n",
    "                     min_count=minimumCount,\n",
    "                     threshold=threshold)\n",
    "    \n",
    "    trigram = Phrases(bigram[words],\n",
    "                      threshold=threshold)  \n",
    "\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    trigram_mod = Phraser(trigram)\n",
    "    return bigram_mod, trigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d553f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertDataToCorpus(cleaned_data):\n",
    "\n",
    "    print('Finished creating first dictionary', datetime.datetime.now())\n",
    "    bigramMod, trigramMod = ngrams(cleaned_data)\n",
    "    print('Finished initializing bi/tri grams', datetime.datetime.now())\n",
    "    ngram =  [trigramMod[bigramMod[review]] for review in cleaned_data]\n",
    "    print('Finished creating bi/tri grams', datetime.datetime.now())\n",
    "       \n",
    "    print('Finished creating first dictionary', datetime.datetime.now())\n",
    "    id2word = gensim.corpora.Dictionary(cleaned_data)\n",
    "    id2word.filter_extremes(no_below=100, no_above=0.5)\n",
    "    \n",
    "    id2word.compactify()\n",
    "    \n",
    "    bow = [id2word.doc2bow(text) for text in cleaned_data]\n",
    "    \n",
    "    print('Finished creating bag of words', datetime.datetime.now())\n",
    "    \n",
    "    return bow, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dead95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-15 13:46:00.441809\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import copy\n",
    "print(datetime.datetime.now())\n",
    "startDate = datetime.date(2020,1, 1)\n",
    "endDate = datetime.date(2021, 1, 1)\n",
    "path = '/Volumes/External HD/2022-02-07/'\n",
    "\n",
    "\n",
    "allpdfs=GetData(startDate, endDate,path)\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(allpdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af520457",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmedStopWords = {}\n",
    "\n",
    "for word in stopwords.words():\n",
    "    stemmedStopWords[ps.stem(word)] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26828ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for vocabulary words with it's index and count\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "X  = pd.DataFrame(allpdfs, columns=[\"content\"])\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "cleanedData = dataCleaning(X)\n",
    "\n",
    "X = cleanedData[\"content\"]\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus, globaldictionary = ConvertDataToCorpus(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmedStopWords = {}\n",
    "\n",
    "# for word in stopwords.words():\n",
    "#     stemmedStopWords[ps.stem(word)] = 0\n",
    "\n",
    "# toRemove = []\n",
    "# for k, v in globaldictionary.items():\n",
    "#     if(v in stemmedStopWords):\n",
    "#         toRemove.append[k]\n",
    "#     else:     \n",
    "#         gDictionary[v.lower()] = 0\n",
    "        \n",
    "# print(len(globaldictionary))\n",
    "# globaldictionary.filter_tokens(bad_ids=toRemove)\n",
    "# globaldictionary.compactify()\n",
    "# print(len(globaldictionary))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the new dictionary and remove stop words, short words, words that aren't alpha numeric and lemmatize\n",
    "import nltk\n",
    "# english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "# new_english_vocab = {}\n",
    "\n",
    "# print('Creating english dictionary', datetime.datetime.now())\n",
    "# for word in english_vocab:\n",
    "#     new_english_vocab[ps.stem(word)] = 0\n",
    "\n",
    "# print('Finished english dictionary', datetime.datetime.now())\n",
    "gDictionary = {}\n",
    "wordsToRemove = []\n",
    "for k, v in globaldictionary.items():\n",
    "#    if(v in new_english_vocab):\n",
    "    gDictionary[v.lower()] = 0\n",
    "        \n",
    "\n",
    "print('Finished cleaning dictionary', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fd6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toKeep = []\n",
    "# for k, v in gDictionary.items():\n",
    "#     toKeep.append(globaldictionary.token2id[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840a3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d8db3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(globaldictionary))\n",
    "# globaldictionary.filter_tokens(good_ids=toKeep)\n",
    "# print(len(globaldictionary))\n",
    "\n",
    "for k, v in globaldictionary.items():\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [globaldictionary.doc2bow(doc) for doc in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240c1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815af736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff445579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2abab3a0",
   "metadata": {},
   "source": [
    "2 -0.9257380753909639\n",
    "3 -0.8924911617047728\n",
    "4 -0.8770794330024948\n",
    "5 -0.8871086585216401\n",
    "6 -0.90274696657373\n",
    "7 -0.9096775832232503\n",
    "8 -0.8935286011309913\n",
    "9 -0.9039521317399287\n",
    "10 -0.8986779384062977\n",
    "11 -0.9037140381713342\n",
    "12 -0.9141724166730084\n",
    "13 -0.9029035359997102\n",
    "14 -0.911527758475253\n",
    "15 -0.8950295260971027\n",
    "16 -0.8968096791358515\n",
    "17 -0.9066689359608383\n",
    "18 -0.9053312826265838\n",
    "19 -0.9105579896968713\n",
    "20 -0.904133068759054\n",
    "21 -0.9121805585867648\n",
    "22 -0.9072281446347834\n",
    "23 -0.9119270569951463\n",
    "24 -0.9145402880275769\n",
    "25 -0.9162855533727349\n",
    "26 -0.9177669915896141\n",
    "27 -0.9217723688770518\n",
    "28 -0.9194462575804779\n",
    "29 -0.9177366729866188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70dfd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from gensim import corpora  \n",
    "\n",
    "dirichlet_dict = globaldictionary\n",
    "bow_corpus = [dirichlet_dict.doc2bow(text) for text in X]\n",
    "\n",
    "# Considering 1-15 topics, as the last is cut off\n",
    "num_topics = list(range(31)[1:])\n",
    "num_keywords = 30\n",
    "\n",
    "LDA_models = {}\n",
    "LDA_topics = {}\n",
    "for i in num_topics:\n",
    "    print(i)\n",
    "    LDA_models[i] = LdaModel(corpus=bow_corpus,\n",
    "                             id2word=dirichlet_dict,\n",
    "                             num_topics=i,\n",
    "                             update_every=1,\n",
    "                             chunksize=len(bow_corpus),\n",
    "                             passes=20,\n",
    "                             alpha='auto',\n",
    "                             random_state=42)\n",
    "\n",
    "    shown_topics = LDA_models[i].show_topics(num_topics=i, \n",
    "                                             num_words=num_keywords,\n",
    "                                             formatted=False)\n",
    "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]\n",
    "    \n",
    "LDA_stability = {}\n",
    "for i in range(0, len(num_topics)-1):\n",
    "    jaccard_sims = []\n",
    "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]): # pylint: disable=unused-variable\n",
    "        sims = []\n",
    "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]): # pylint: disable=unused-variable\n",
    "            sims.append(jaccard_similarity(topic1, topic2))    \n",
    "        \n",
    "        jaccard_sims.append(sims)    \n",
    "    \n",
    "    LDA_stability[num_topics[i]] = jaccard_sims\n",
    "                \n",
    "mean_stabilities = [np.array(LDA_stability[i]).mean() for i in num_topics[:-1]]\n",
    "\n",
    "coherences = [CoherenceModel(model=LDA_models[i], texts=corpus, dictionary=dirichlet_dict, coherence='c_v').get_coherence()\\\n",
    "              for i in num_topics[:-1]]\n",
    "\n",
    "coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(num_keywords)[:-1]] # limit topic numbers to the number of keywords\n",
    "coh_sta_max = max(coh_sta_diffs)\n",
    "coh_sta_max_idxs = [i for i, j in enumerate(coh_sta_diffs) if j == coh_sta_max]\n",
    "ideal_topic_num_index = coh_sta_max_idxs[0] # choose less topics in case there's more than one max\n",
    "ideal_topic_num = num_topics[ideal_topic_num_index]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=mean_stabilities, label='Average Topic Overlap')\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
    "\n",
    "ax.axvline(x=ideal_topic_num, label='Ideal Number of Topics', color='black')\n",
    "ax.axvspan(xmin=ideal_topic_num - 1, xmax=ideal_topic_num + 1, alpha=0.5, facecolor='grey')\n",
    "\n",
    "y_max = max(max(mean_stabilities), max(coherences)) + (0.10 * max(max(mean_stabilities), max(coherences)))\n",
    "ax.set_ylim([0, y_max])\n",
    "ax.set_xlim([1, num_topics[-1]-1])\n",
    "                \n",
    "ax.axes.set_title('Model Metrics per Number of Topics', fontsize=25)\n",
    "ax.set_ylabel('Metric Level', fontsize=20)\n",
    "ax.set_xlabel('Number of Topics', fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = CreateLdaModel(20)\n",
    "\n",
    "lda_model.top_topics(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get data for a time slice\n",
    "print(datetime.datetime.now())\n",
    "startDate = datetime.date(2020,1, 1)\n",
    "endDate = datetime.date(2020, 2, 1)\n",
    "path = '/Volumes/External HD/2022-02-07/'\n",
    "\n",
    "\n",
    "listOfpdfs2=GetData(startDate, endDate,path)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0027bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(datetime.datetime.now())\n",
    "# Create a dictionary for vocabulary words with it's index and count\n",
    "XX  = pd.DataFrame(listOfpdfs2, columns=[\"content\"])\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "cleanedData = dataCleaning(XX)\n",
    "\n",
    "XX = cleanedData[\"content\"]\n",
    "\n",
    "print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XX = []\n",
    "# count = 0 \n",
    "# for l in cleanedData[\"content\"]:\n",
    "#     XX.append(l.split())\n",
    "\n",
    "bow_corpus, dictionary = ConvertDataToCorpus(XX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary for vocabulary words with it's index and count\n",
    "# dictionary = gensim.corpora.Dictionary(XX)\n",
    "\n",
    "\n",
    "# # filter words that occurs in less than 5 documents and words that occurs in more than 50% of total documents\n",
    "# # keep top 100000 frequent words\n",
    "# #dictionary.filter_extremes(no_below=5, no_above=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #iterate through the new dictionary and remove stop words, short words, words that aren't alpha numeric and lemmatize\n",
    "# print(dictionary[0])\n",
    "# goodTokens = []\n",
    "# goodTokensFromLocalDictionary = []\n",
    "# for k, v in dictionary.items():\n",
    "#     if v in stopwords.words():\n",
    "#         continue\n",
    "#     goodTokensFromLocalDictionary.append(v)       \n",
    "        \n",
    "# for k, v in gDictionary.items():\n",
    "#     if(k in goodTokensFromLocalDictionary):\n",
    "#         goodTokens.append(dictionary.token2id[k])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e81fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dictionary))\n",
    "# dictionary.filter_tokens(good_ids=goodTokens)\n",
    "# print(len(dictionary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dictionary))\n",
    "print(len(gDictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97717684",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in XX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from gensim import corpora  \n",
    "\n",
    "dirichlet_dict = globaldictionary\n",
    "bow_corpus = [dirichlet_dict.doc2bow(text) for text in XX]\n",
    "\n",
    "# Considering 1-15 topics, as the last is cut off\n",
    "num_topics = list(range(31)[1:])\n",
    "num_keywords = 30\n",
    "\n",
    "LDA_models = {}\n",
    "LDA_topics = {}\n",
    "for i in num_topics:\n",
    "    print(i)\n",
    "    LDA_models[i] = LdaModel(corpus=bow_corpus,\n",
    "                             id2word=dirichlet_dict,\n",
    "                             num_topics=i,\n",
    "                             update_every=1,\n",
    "                             chunksize=len(bow_corpus),\n",
    "                             passes=20,\n",
    "                             alpha='auto',\n",
    "                             random_state=42)\n",
    "\n",
    "    shown_topics = LDA_models[i].show_topics(num_topics=i, \n",
    "                                             num_words=num_keywords,\n",
    "                                             formatted=False)\n",
    "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]\n",
    "    \n",
    "LDA_stability = {}\n",
    "for i in range(0, len(num_topics)-1):\n",
    "    jaccard_sims = []\n",
    "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]): # pylint: disable=unused-variable\n",
    "        sims = []\n",
    "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]): # pylint: disable=unused-variable\n",
    "            sims.append(jaccard_similarity(topic1, topic2))    \n",
    "        \n",
    "        jaccard_sims.append(sims)    \n",
    "    \n",
    "    LDA_stability[num_topics[i]] = jaccard_sims\n",
    "                \n",
    "mean_stabilities = [np.array(LDA_stability[i]).mean() for i in num_topics[:-1]]\n",
    "\n",
    "coherences = [CoherenceModel(model=LDA_models[i], texts=corpus, dictionary=dirichlet_dict, coherence='c_v').get_coherence()\\\n",
    "              for i in num_topics[:-1]]\n",
    "\n",
    "coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(num_keywords)[:-1]] # limit topic numbers to the number of keywords\n",
    "coh_sta_max = max(coh_sta_diffs)\n",
    "coh_sta_max_idxs = [i for i, j in enumerate(coh_sta_diffs) if j == coh_sta_max]\n",
    "ideal_topic_num_index = coh_sta_max_idxs[0] # choose less topics in case there's more than one max\n",
    "ideal_topic_num = num_topics[ideal_topic_num_index]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=mean_stabilities, label='Average Topic Overlap')\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
    "\n",
    "ax.axvline(x=ideal_topic_num, label='Ideal Number of Topics', color='black')\n",
    "ax.axvspan(xmin=ideal_topic_num - 1, xmax=ideal_topic_num + 1, alpha=0.5, facecolor='grey')\n",
    "\n",
    "y_max = max(max(mean_stabilities), max(coherences)) + (0.10 * max(max(mean_stabilities), max(coherences)))\n",
    "ax.set_ylim([0, y_max])\n",
    "ax.set_xlim([1, num_topics[-1]-1])\n",
    "                \n",
    "ax.axes.set_title('Model Metrics per Number of Topics', fontsize=25)\n",
    "ax.set_ylabel('Metric Level', fontsize=20)\n",
    "ax.set_xlabel('Number of Topics', fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b40002",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model2 = gensim.models.LdaModel(corpus=bow_corpus,\n",
    "                                   id2word=dictionary,\n",
    "                                   num_topics=2,\n",
    "                                   chunksize=len(bow_corpus),\n",
    "                                   alpha='auto',\n",
    "                                   eta='auto',\n",
    "                                   offset=0.75,\n",
    "                                   passes=10,\n",
    "                                   random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model2.show_topic(0, len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseVector = []\n",
    "for n in range(0,len(gDictionary)):\n",
    "    baseVector.append(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "# import copy\n",
    "# d2 = copy.deepcopy(d)\n",
    "timesliceDictionaries = []\n",
    "for n in range(0,2):\n",
    "    for word in lda_model2.show_topic(n, len(dictionary)):\n",
    "        if word[0] in gDictionary:\n",
    "            gDictionary[word[0]] = word[1]\n",
    "    timesliceDictionaries.append(copy.deepcopy(gDictionary))\n",
    "\n",
    "    for k, v in gDictionary.items():\n",
    "        gDictionary[k] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make base vector from global dictionary\n",
    "#loop over number of topics chosen from the whole corpus and initlaize the a vector of vectors\n",
    "#loop over number of topics from the timeslice and intialize in vector of vectors\n",
    "#nested for loop with both vectors and compare cosine simularity\n",
    "#graph each point in a graph with the overall having a different color than \n",
    "\n",
    "len(timesliceDictionaries)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusDictionaries = []\n",
    "for n in range(0,10):\n",
    "    for word in lda_model.show_topic(n, len(gDictionary)):\n",
    "        if word[0] in gDictionary:\n",
    "            gDictionary[word[0]] = word[1]\n",
    "    corpusDictionaries.append(copy.deepcopy(gDictionary))\n",
    "\n",
    "    for k, v in gDictionary.items():\n",
    "        gDictionary[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacde168",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(corpusDictionaries[0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d0fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import pprint\n",
    "\n",
    "results = []\n",
    "for topic in corpusDictionaries:\n",
    "    rowResults = []\n",
    "    for topic2 in timesliceDictionaries:\n",
    "        rowResults.append(1-spatial.distance.cosine(list(topic.values()), list(topic2.values())))\n",
    "        #print(spatial.distance.cosine(list(topic.values()), list(topic2.values())))\n",
    "    results.append(rowResults)\n",
    "    #print('\\n')\n",
    "    \n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540dedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf6e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e7259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb41a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
